t1 = "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä" #‚Üí ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
t2 = "hello,world!!!" #‚Üí ["hello", "world"]
t3 = "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ" #‚Üí ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
t4 = "2025 –≥–æ–¥" #‚Üí ["2025", "–≥–æ–¥"]
t5 = "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ" #‚Üí ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"] (—ç–º–æ–¥–∑–∏ –≤—ã–ø–∞–¥–∞—é—Ç)


def normalize1(n):
    import re
    n = n.casefold().strip()
    s = re.sub(r'[^0-9—ëa-zA-Z–∞-—è–ê-–Ø-]', ' ', n)
    s = s.replace('—ë', '–µ')
    s = s.split()
    s = ' '.join(s)
    return s

def tokenize(n):
    s = normalize1(n)
    return s.split(' ')

print(tokenize(t1))
print(tokenize(t2))
print(tokenize(t3))
print(tokenize(t4))
print(tokenize(t5))

# ['–ø—Ä–∏–≤–µ—Ç', '–º–∏—Ä']
# ['hello', 'world']
# ['–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É', '–∫—Ä—É—Ç–æ']
# ['–≥–æ–¥']
# ['emoji', '–Ω–µ', '—Å–ª–æ–≤–æ']